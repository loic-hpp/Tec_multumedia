{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install opencv-python\n",
    "%pip install -U matplotlib\n",
    "%pip install einops\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from einops import rearrange\n",
    "from PIL import Image # Using here to convert openCV image to PIL image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "STEP_TO_NEXT_FRAME = 16 \n",
    "\"\"\" in each video, we will read 1 frame every 24/STEP_TO_INDEX frames\"\"\"\n",
    "DEBUG = False # if True, print debug information\n",
    "NUMBER_OF_VIDEO = 100 # number of videos to index\n",
    "VIDEOS_SIZE_ON_DISK = 444039168  # in octets\n",
    "MAX_FRAME_NUMBER = 37895 # maximum number of frames in all video\n",
    "N = int(MAX_FRAME_NUMBER/STEP_TO_NEXT_FRAME) # index Length\n",
    "D = 128 # index Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all global variables\n",
    "imageList = []\n",
    "videoList = []\n",
    "indexationTable = [] # Table of indexation\n",
    "resnet_descriptor = np.empty((N, 512), dtype=np.float16) # Descriptor of the image by resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath = \"/data/jpeg/\"\n",
    "videoPath = \"/data/mp4/\"\n",
    "currDirectory = os.getcwd()\n",
    "imageList = os.listdir(currDirectory + imagePath)\n",
    "videoList = os.listdir(currDirectory + videoPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVideoParameter(videoPath:str):\n",
    "    \"\"\"return the frame rate and the number of frames of the video.\"\"\"\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    return fps, frame_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_to_pil(cv2_img):\n",
    "    \"\"\"Convert openCV image to PIL image to perfom resnet computing\"\"\"\n",
    "    cv2_im_rgb = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(cv2_im_rgb)\n",
    "    return pil_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet-18 model loading\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)   # le modèle est chargé avec des poids pré-entrainés sur ImageNet\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))        # supprime la dernière couche du réseau\n",
    "model.eval()\n",
    "if torch.cuda.is_available():  \n",
    "    model.cuda() # On envoie le modèle sur le GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageVectorResnet_CPU(imagePath, needToRead:bool = True):\n",
    "    \"\"\"Return resnet18 image descriptor\"\"\"\n",
    "    if needToRead:\n",
    "        image = cv2.imread(imagePath)\n",
    "    else:\n",
    "        image = imagePath\n",
    "    \n",
    "    new_image = cv2_to_pil(image)\n",
    "    \n",
    "    # Pre-processing\n",
    "    preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                       \n",
    "        transforms.ToTensor(),                            \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],    \n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    input_tensor = preprocess(new_image)         # 3 x 224 x 224\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Ajout d'une dimension de batch : 1 x 3 x 224 x 224\n",
    "    \n",
    "    if torch.cuda.is_available():  \n",
    "        input_batch = input_batch.cuda() # On envoie le batch sur le GPU\n",
    "    \n",
    "    # Computing descriptor\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)  # 1 x 512 x 1 x 1 \n",
    "\n",
    "\n",
    "    output = rearrange(output, 'b d h w -> (b d h w)')  # 512\n",
    "    output2_list = output.tolist()\n",
    "    return output2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotingResnetDescriptor(descriptor:list, index_desc:int = 0):\n",
    "    plt.plot(descriptor[index_desc])\n",
    "    plt.title(\"Resnet descriptor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndexTableResnet():\n",
    "    \"\"\"Index a video by creating a list of frames and their corresponding index. and update the indexationTable variable.\"\"\"\n",
    "    for video in videoList:\n",
    "        path = currDirectory + videoPath + video\n",
    "        fps, frame_count= getVideoParameter(path)\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        startIndex = len(indexationTable)\n",
    "        stopIndex = startIndex - 1 + math.floor(frame_count/STEP_TO_NEXT_FRAME)\n",
    "        for i in range(startIndex, stopIndex):\n",
    "            frameNumber = STEP_TO_NEXT_FRAME*(i-startIndex)\n",
    "            indexationTable.append((frameNumber, video, (frameNumber/fps)))\n",
    "            currImage = cap.set(cv2.CAP_PROP_POS_FRAMES, frameNumber)\n",
    "            ret, currImage = cap.read()\n",
    "            resnet_descriptor[i] = ImageVectorResnet_CPU(currImage, False)\n",
    "            \n",
    " \n",
    "createIndexTableResnet()       \n",
    "plotingResnetDescriptor(resnet_descriptor, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateCompressionRate():\n",
    "    rate = 1- (512*len(indexationTable)*2)/VIDEOS_SIZE_ON_DISK #*2 because we use float16\n",
    "    print(\"Le taux de compression est: \",rate)\n",
    "    \n",
    "CalculateCompressionRate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchImage():\n",
    "    SearchResult = []\n",
    "    bestIndex = 0\n",
    "    for image in imageList:\n",
    "        path = currDirectory + imagePath + image\n",
    "        imageVector = ImageVectorResnet_CPU(path)\n",
    "        for i in range(len(indexationTable)):\n",
    "            # calculer la norme de la différence entre l'image et la liste de descripteurs\n",
    "            dist = np.linalg.norm(imageVector - resnet_descriptor[i])\n",
    "            if i == 0:\n",
    "                bestDistance = dist\n",
    "                bestIndex = i\n",
    "            if dist < bestDistance:\n",
    "                bestDistance = dist\n",
    "                bestIndex = i\n",
    "\n",
    "        if bestDistance < 14.89:\n",
    "            bestVideo = indexationTable[bestIndex][1]\n",
    "            bestTime = indexationTable[bestIndex][2]\n",
    "        else:\n",
    "            # print(image.split(\".\")[0], \" \", bestDistance)\n",
    "            bestVideo = \"out\"\n",
    "            bestTime = \"\"\n",
    "        # print(\"The best match is the frame number \", (image.split(\".\")[0], bestVideo.split(\".\")[0], bestTime)) \n",
    "        if type(bestTime) == float: \n",
    "            SearchResult.append((image.split(\".\")[0], bestVideo.split(\".\")[0], round(bestTime, 3)))\n",
    "        else:\n",
    "            SearchResult.append((image.split(\".\")[0], bestVideo.split(\".\")[0], bestTime)) \n",
    "        # break\n",
    "    return SearchResult\n",
    "\n",
    "SearchResult = searchImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResultInCSVFile():\n",
    "    \"\"\"Function to write the results in a csv file.\"\"\"\n",
    "    with open(currDirectory+'/src/test.csv', 'w') as file:\n",
    "        file.write(\"image,video_pred,minutage_pred\\n\")\n",
    "        for imageName, video, time in SearchResult:\n",
    "            file.write(imageName + \",\" + video + \",\" + str(time) + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "writeResultInCSVFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "import subprocess\n",
    "directory = r'C:\\Users\\loicn\\OneDrive - polymtl.ca\\Polymtl\\Session_5\\INF8770_TEC_MULTIMEDIA\\Lab\\Lab3\\src'\n",
    "output = subprocess.run(['python3', 'evaluate.py', '--file', 'test.csv', '--file_gt', '../data/gt.csv'], cwd=directory, capture_output=True, text=True)\n",
    "\n",
    "print(output.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
